{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:106: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, kernel_initializer=\"lecun_uniform\")`\n",
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:108: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, return_sequences=True, kernel_initializer=\"lecun_uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 50, 600), dtype=float32)\n",
      "Tensor(\"dense_2/Softmax:0\", shape=(?, 7000), dtype=float32)\n",
      "Number of exemples = 6821\n",
      "77925\n",
      "2181900000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:147: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 0, training examples: 0 - 6821\n",
      "WARNING:tensorflow:From C:\\Users\\sriha\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[30000,3500] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/mul_52}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-936c886dfcca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training epoch: %d, training examples: %d - %d'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mtest_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m41\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[30000,3500] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/mul_52}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from keras.layers import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, Add, Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from attention import AttentionLayer\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "np.random.seed(1234)  # for reproducibility\n",
    "import pickle as cPickle\n",
    "#import theano.tensor as T\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "word_embedding_size = 100\n",
    "sentence_embedding_size = 300\n",
    "dictionary_size = 7000\n",
    "#dictionary_size = 4515\n",
    "maxlen_input = 50\n",
    "maxlen_output = 50\n",
    "num_subsets = 1\n",
    "Epochs = 100\n",
    "BatchSize = 64  #  Check the capacity of your GPU\n",
    "#BatchSize = 1\n",
    "Patience = 0\n",
    "dropout = .25\n",
    "n_test = 100\n",
    "\n",
    "vocabulary_file = 'vocabulary_wiki_keyw'\n",
    "#vocabulary_file = 'vocabulary_movie'\n",
    "questions_file = 'Padded_context'\n",
    "answers_file = 'Padded_answers'\n",
    "weights_file = 'new_model_weights_test_att.h5'\n",
    "GLOVE_DIR = './'\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=Patience)\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "\n",
    "def print_result(input):\n",
    "\n",
    "    ans_partial = np.zeros((1,maxlen_input))\n",
    "    ans_partial[0, -1] = 2  #  the index of the symbol BOS (begin of sentence)\n",
    "    for k in range(maxlen_input - 1):\n",
    "        ye = model.predict([input, ans_partial])\n",
    "        mp = np.argmax(ye)\n",
    "        ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
    "        ans_partial[0, -1] = mp\n",
    "    text = ''\n",
    "    for k in ans_partial[0]:\n",
    "        k = k.astype(int)\n",
    "        if k < (dictionary_size-2):\n",
    "            w = vocabulary[k]\n",
    "            text = text + w[0] + ' '\n",
    "    return(text)\n",
    "\n",
    "\n",
    "# **********************************************************************\n",
    "# Reading a pre-trained word embedding and addapting to our vocabulary:\n",
    "# **********************************************************************\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.zeros((dictionary_size, word_embedding_size))\n",
    "\n",
    "# Loading our vocabulary:\n",
    "vocabulary = cPickle.load(open(vocabulary_file, 'rb'))\n",
    "\n",
    "# Using the Glove embedding:\n",
    "i = 0\n",
    "for word in vocabulary:\n",
    "    embedding_vector = embeddings_index.get(word[0])\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    i += 1\n",
    "\n",
    "# *******************************************************************\n",
    "# Keras model of the chatbot: \n",
    "# *******************************************************************\n",
    "\n",
    "ad = Adam(lr=0.00005) \n",
    "\n",
    "input_context = Input(shape=(maxlen_input,), dtype='int32', name='input_context')\n",
    "input_answer = Input(shape=(maxlen_input,), dtype='int32', name='input_answer')\n",
    "\n",
    "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform', return_sequences=True)\n",
    "\n",
    "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform', return_sequences=True)\n",
    "\n",
    "if os.path.isfile(weights_file):\n",
    "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
    "else:\n",
    "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
    "word_embedding_context = Shared_Embedding(input_context)\n",
    "context_embedding_out = LSTM_encoder(word_embedding_context)\n",
    "\n",
    "\n",
    "word_embedding_answer = Shared_Embedding(input_answer)\n",
    "answer_embedding_out = LSTM_decoder(word_embedding_answer)\n",
    "\n",
    "# Attention layer\n",
    "\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "answer_temp = answer_embedding_out\n",
    "attn_out, attn_states = attn_layer([context_embedding_out, answer_embedding_out])\n",
    "\n",
    "merge_layer = Concatenate()([answer_embedding_out, attn_out])\n",
    "print(merge_layer)\n",
    "merge_layer = Flatten()(merge_layer)\n",
    "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
    "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
    "\n",
    "print(out)\n",
    "\n",
    "model = Model(input=[input_context, input_answer], output=[out])\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
    "\n",
    "if os.path.isfile(weights_file):\n",
    "    model.load_weights(weights_file)\n",
    "\n",
    "# ************************************************************************\n",
    "# Loading the data:\n",
    "# ************************************************************************\n",
    "\n",
    "q = cPickle.load(open(questions_file, 'rb'))\n",
    "a = cPickle.load(open(answers_file, 'rb'))\n",
    "n_exem, n_words = a.shape\n",
    "\n",
    "qt = q[0:n_test,:]\n",
    "at = a[0:n_test,:]\n",
    "q = q[n_test + 1:,:]\n",
    "a = a[n_test + 1:,:]\n",
    "\n",
    "print('Number of exemples = %d'%(n_exem - n_test))\n",
    "step = np.around(int((n_exem - n_test)/num_subsets))\n",
    "round_exem = step * num_subsets\n",
    "\n",
    "# *************************************************************************\n",
    "# Bot training:\n",
    "# *************************************************************************\n",
    "\n",
    "x = range(0,Epochs) \n",
    "valid_loss = np.zeros(Epochs)\n",
    "train_loss = np.zeros(Epochs)\n",
    "for m in range(Epochs):\n",
    "    \n",
    "    # Loop over training batches due to memory constraints:\n",
    "    for n in range(0,round_exem,step):\n",
    "        \n",
    "        q2 = q[n:n+step]\n",
    "        s = q2.shape\n",
    "        count = 0\n",
    "        for i, sent in enumerate(a[n:n+step]):\n",
    "            l = np.where(sent==2)  #  the position od the symbol EOS\n",
    "            limit = l[0][0]\n",
    "            count += limit + 1\n",
    "        print(count)    \n",
    "        Q = np.zeros((count,maxlen_input))\n",
    "        A = np.zeros((count,maxlen_input))\n",
    "        Y = np.zeros((count,dictionary_size),dtype='float32')\n",
    "        print(Y.nbytes)\n",
    "        \n",
    "        # Loop over the training examples:\n",
    "        count = 0\n",
    "        for i, sent in enumerate(a[n:n+step]):\n",
    "            ans_partial = np.zeros((1,maxlen_input))\n",
    "            \n",
    "            # Loop over the positions of the current target output (the current output sequence):\n",
    "            l = np.where(sent==2)  #  the position of the symbol EOS\n",
    "            limit = l[0][0]\n",
    "\n",
    "            for k in range(1,limit+1):\n",
    "                # Mapping the target output (the next output word) for one-hot codding:\n",
    "                y = np.zeros((1, dictionary_size))\n",
    "                y[0, sent[k]] = 1\n",
    "\n",
    "                # preparing the partial answer to input:\n",
    "\n",
    "                ans_partial[0,-k:] = sent[0:k]\n",
    "\n",
    "                # training the model for one epoch using teacher forcing:\n",
    "                \n",
    "                Q[count, :] = q2[i:i+1] \n",
    "                A[count, :] = ans_partial \n",
    "                Y[count, :] = y\n",
    "                count += 1\n",
    "                \n",
    "        print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
    "        model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
    "         \n",
    "        test_input = qt[41:42]\n",
    "        print(print_result(test_input))\n",
    "        train_input = q[41:42]\n",
    "        print(print_result(train_input))        \n",
    "        \n",
    "    model.save_weights(weights_file, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
