{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the context data...\n",
      "Reading the answer data...\n",
      "Tokenazing the answers...\n",
      "Using vocabulary of size 7000.\n",
      "The least frequent word in our vocabulary is 'airlock' and appeared 80 times.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(1234)  # for reproducibility\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import itertools\n",
    "import operator\n",
    "import pickle\n",
    "import numpy as np    \n",
    "from keras.preprocessing import sequence\n",
    "from scipy import sparse, io\n",
    "from numpy.random import permutation\n",
    "import re\n",
    "    \n",
    "questions_file = 'context'\n",
    "answers_file = 'answers'\n",
    "vocabulary_file = 'vocabulary_movie'\n",
    "padded_questions_file = 'Padded_context'\n",
    "padded_answers_file = 'Padded_answers'\n",
    "unknown_token = 'something'\n",
    "\n",
    "vocabulary_size = 7000\n",
    "max_features = vocabulary_size\n",
    "maxlen_input = 50\n",
    "maxlen_output = 50  # cut texts after this number of words\n",
    "\n",
    "print (\"Reading the context data...\")\n",
    "q = open(questions_file, 'r')\n",
    "questions = q.read()\n",
    "print (\"Reading the answer data...\")\n",
    "a = open(answers_file, 'r')\n",
    "answers = a.read()\n",
    "all = answers + questions\n",
    "print (\"Tokenazing the answers...\")\n",
    "paragraphs_a = [p for p in answers.split('\\n')]\n",
    "paragraphs_b = [p for p in all.split('\\n')]\n",
    "paragraphs_a = ['BOS '+p+' EOS' for p in paragraphs_a]\n",
    "paragraphs_b = ['BOS '+p+' EOS' for p in paragraphs_b]\n",
    "paragraphs_b = ' '.join(paragraphs_b)\n",
    "tokenized_text = paragraphs_b.split()\n",
    "paragraphs_q = [p for p in questions.split('\\n') ]\n",
    "tokenized_answers = [p.split() for p in paragraphs_a]\n",
    "tokenized_questions = [p.split() for p in paragraphs_q]\n",
    "\n",
    "\n",
    "vocab = pickle.load(open(vocabulary_file, 'rb'))\n",
    "\n",
    "\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print (\"Using vocabulary of size %d.\" % vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replacing all words not in our vocabulary with the unknown token:\n",
    "for i, sent in enumerate(tokenized_answers):\n",
    "    tokenized_answers[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "   \n",
    "for i, sent in enumerate(tokenized_questions):\n",
    "    tokenized_questions[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "# Creating the training data:\n",
    "X = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_questions])\n",
    "Y = np.asarray([[word_to_index[w] for w in sent] for sent in tokenized_answers])\n",
    "\n",
    "Q = sequence.pad_sequences(X, maxlen=maxlen_input)\n",
    "A = sequence.pad_sequences(Y, maxlen=maxlen_output, padding='post')\n",
    "\n",
    "with open(padded_questions_file, 'wb') as q:\n",
    "    pickle.dump(Q, q)\n",
    "    \n",
    "with open(padded_answers_file, 'wb') as a:\n",
    "    pickle.dump(A, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
