{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:104: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(300, kernel_initializer=\"lecun_uniform\")`\n",
      "C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\ipykernel_launcher.py:119: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of exemples = 61600\n",
      "22294888000\n",
      "Training epoch: 0, training examples: 0 - 61600\n",
      "WARNING:tensorflow:From C:\\Users\\Nripesh\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n",
      " 72640/796246 [=>............................] - ETA: 13:37 - loss: 4.5815"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a0451c4a6336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training epoch: %d, training examples: %d - %d'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBatchSize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mtest_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m41\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[0;32m    185\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gputest\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    553\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from keras.layers import *\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, RepeatVector, Bidirectional, Dropout, Add\n",
    "from keras.layers import Concatenate\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "np.random.seed(1234)  # for reproducibility\n",
    "import pickle as cPickle\n",
    "#import theano.tensor as T\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word_embedding_size = 100\n",
    "sentence_embedding_size = 300\n",
    "dictionary_size = 7000\n",
    "maxlen_input = 50\n",
    "maxlen_output = 50\n",
    "num_subsets = 1\n",
    "Epochs = 100\n",
    "BatchSize = 64  #  Check the capacity of your GPU\n",
    "Patience = 0\n",
    "dropout = .25\n",
    "n_test = 100\n",
    "\n",
    "vocabulary_file = 'vocabulary_movie'\n",
    "questions_file = 'Padded_context'\n",
    "answers_file = 'Padded_answers'\n",
    "weights_file = 'new_model_weights20.h5'\n",
    "GLOVE_DIR = './'\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=Patience)\n",
    "\n",
    "\n",
    "\n",
    "def print_result(input):\n",
    "\n",
    "    ans_partial = np.zeros((1,maxlen_input))\n",
    "    ans_partial[0, -1] = 2  #  the index of the symbol BOS (begin of sentence)\n",
    "    for k in range(maxlen_input - 1):\n",
    "        ye = model.predict([input, ans_partial])\n",
    "        mp = np.argmax(ye)\n",
    "        ans_partial[0, 0:-1] = ans_partial[0, 1:]\n",
    "        ans_partial[0, -1] = mp\n",
    "    text = ''\n",
    "    for k in ans_partial[0]:\n",
    "        k = k.astype(int)\n",
    "        if k < (dictionary_size-2):\n",
    "            w = vocabulary[k]\n",
    "            text = text + w[0] + ' '\n",
    "    return(text)\n",
    "\n",
    "\n",
    "# **********************************************************************\n",
    "# Reading a pre-trained word embedding and addapting to our vocabulary:\n",
    "# **********************************************************************\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "embedding_matrix = np.zeros((dictionary_size, word_embedding_size))\n",
    "\n",
    "# Loading our vocabulary:\n",
    "vocabulary = cPickle.load(open(vocabulary_file, 'rb'))\n",
    "\n",
    "# Using the Glove embedding:\n",
    "i = 0\n",
    "for word in vocabulary:\n",
    "    embedding_vector = embeddings_index.get(word[0])\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    i += 1\n",
    "\n",
    "# *******************************************************************\n",
    "# Keras model of the chatbot: \n",
    "# *******************************************************************\n",
    "\n",
    "ad = Adam(lr=0.00005) \n",
    "\n",
    "input_context = Input(shape=(maxlen_input,), dtype='int32', name='input_context')\n",
    "input_answer = Input(shape=(maxlen_input,), dtype='int32', name='input_answer')\n",
    "LSTM_encoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
    "LSTM_decoder = LSTM(sentence_embedding_size, init= 'lecun_uniform')\n",
    "if os.path.isfile(weights_file):\n",
    "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, input_length=maxlen_input)\n",
    "else:\n",
    "    Shared_Embedding = Embedding(output_dim=word_embedding_size, input_dim=dictionary_size, weights=[embedding_matrix], input_length=maxlen_input)\n",
    "word_embedding_context = Shared_Embedding(input_context)\n",
    "context_embedding = LSTM_encoder(word_embedding_context)\n",
    "\n",
    "word_embedding_answer = Shared_Embedding(input_answer)\n",
    "answer_embedding = LSTM_decoder(word_embedding_answer)\n",
    "\n",
    "merge_layer = Concatenate()([context_embedding, answer_embedding])\n",
    "out = Dense(int(dictionary_size/2), activation=\"relu\")(merge_layer)\n",
    "out = Dense(dictionary_size, activation=\"softmax\")(out)\n",
    "\n",
    "model = Model(input=[input_context, input_answer], output = [out])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ad)\n",
    "\n",
    "if os.path.isfile(weights_file):\n",
    "    model.load_weights(weights_file)\n",
    "\n",
    "# ************************************************************************\n",
    "# Loading the data:\n",
    "# ************************************************************************\n",
    "\n",
    "q = cPickle.load(open(questions_file, 'rb'))\n",
    "a = cPickle.load(open(answers_file, 'rb'))\n",
    "n_exem, n_words = a.shape\n",
    "\n",
    "qt = q[0:n_test,:]\n",
    "at = a[0:n_test,:]\n",
    "q = q[n_test + 1:,:]\n",
    "a = a[n_test + 1:,:]\n",
    "\n",
    "print('Number of exemples = %d'%(n_exem - n_test))\n",
    "step = np.around(int((n_exem - n_test)/num_subsets))\n",
    "round_exem = step * num_subsets\n",
    "\n",
    "# *************************************************************************\n",
    "# Bot training:\n",
    "# *************************************************************************\n",
    "\n",
    "x = range(0,Epochs) \n",
    "valid_loss = np.zeros(Epochs)\n",
    "train_loss = np.zeros(Epochs)\n",
    "for m in range(Epochs):\n",
    "    \n",
    "    # Loop over training batches due to memory constraints:\n",
    "    for n in range(0,round_exem,step):\n",
    "        \n",
    "        q2 = q[n:n+step]\n",
    "        s = q2.shape\n",
    "        count = 0\n",
    "        for i, sent in enumerate(a[n:n+step]):\n",
    "            l = np.where(sent==3)  #  the position od the symbol EOS\n",
    "            limit = l[0][0]\n",
    "            count += limit + 1\n",
    "            \n",
    "        Q = np.zeros((count,maxlen_input))\n",
    "        A = np.zeros((count,maxlen_input))\n",
    "        Y = np.zeros((count,dictionary_size),dtype='float32')\n",
    "        print(Y.nbytes)\n",
    "        \n",
    "        # Loop over the training examples:\n",
    "        count = 0\n",
    "        for i, sent in enumerate(a[n:n+step]):\n",
    "            ans_partial = np.zeros((1,maxlen_input))\n",
    "            \n",
    "            # Loop over the positions of the current target output (the current output sequence):\n",
    "            l = np.where(sent==3)  #  the position of the symbol EOS\n",
    "            limit = l[0][0]\n",
    "\n",
    "            for k in range(1,limit+1):\n",
    "                # Mapping the target output (the next output word) for one-hot codding:\n",
    "                y = np.zeros((1, dictionary_size))\n",
    "                y[0, sent[k]] = 1\n",
    "\n",
    "                # preparing the partial answer to input:\n",
    "\n",
    "                ans_partial[0,-k:] = sent[0:k]\n",
    "\n",
    "                # training the model for one epoch using teacher forcing:\n",
    "                \n",
    "                Q[count, :] = q2[i:i+1] \n",
    "                A[count, :] = ans_partial \n",
    "                Y[count, :] = y\n",
    "                count += 1\n",
    "                \n",
    "        print('Training epoch: %d, training examples: %d - %d'%(m,n, n + step))\n",
    "        model.fit([Q, A], Y, batch_size=BatchSize, epochs=1)\n",
    "         \n",
    "        test_input = qt[41:42]\n",
    "        print(print_result(test_input))\n",
    "        train_input = q[41:42]\n",
    "        print(print_result(train_input))        \n",
    "        \n",
    "    model.save_weights(weights_file, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
